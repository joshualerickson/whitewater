---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  fig.align = 'center',
  out.width = "75%",
  dev.args = list(png = list(type = "cairo"))
)
```

# whitewater

<!-- badges: start -->
[![R-CMD-check](https://github.com/joshualerickson/whitewater/workflows/R-CMD-check/badge.svg)](https://github.com/joshualerickson/whitewater/actions)[![codecov](https://codecov.io/gh/joshualerickson/whitewater/branch/main/graph/badge.svg)](https://app.codecov.io/gh/joshualerickson/whitewater)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-brightgreen.svg)](https://lifecycle.r-lib.org/articles/stages.html#stable)
<!-- badges: end -->


The goal of whitewater is to provide sequential and parallel processing for USGS stations in a tidy-style format. This package allows user to `plan()` their choice of parallel processing and then use the argument `parallel = TRUE` in whitewater function calls. The package also puts every output in a `tibble` with data munging of sites, parameter and stat codes, which results in a __tidy__ style data frame.  

## Installation

You can install the development version of whitewater from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("joshualerickson/whitewater")
```

## Example

This is a basic example which shows you how to solve a common problem: get daily values of discharge for multiple sites (all active sites in Pacific Northwest (Region 17)) using parallel processing. Please see [furrr](https://cran.r-project.org/web/packages/furrr/index.html) and [future](https://cran.r-project.org/web/packages/future/index.html) for more details on parallel processing methods.  

### Running in parallel  

```{r, warning=F, message=F}
library(whitewater)
library(tidyverse)
library(sf)
library(future)
library(dataRetrieval)

huc17_sites <- dataRetrieval::whatNWISdata(huc = 17,
                                           siteStatus = 'active',
                                           service = 'dv',
                                           parameterCd = '00060',
                                           drainAreaMax = 2000)
cat("# of sites: ", nrow(huc17_sites))

st_as_sf(huc17_sites, coords = c('dec_long_va', 'dec_lat_va')) %>% 
  ggplot() + 
  geom_sf() +
  borders('state', xlim = c(-130, -110), ylim = c(20, 50)) + 
  theme_bw()
```


```{r}
#need to call future::plan()
plan(multisession(workers = availableCores()-1))

#running on 11 cores

system.time({
pnw_dv <- suppressMessages(ww_dvUSGS(huc17_sites$site_no,
                    parameter_cd = '00060',
                    wy_month = 10,
                    parallel = TRUE))
})

nrow(pnw_dv)
```

Now we can use other `ww_` functions to filter the data by water year, month, water year and month, as well as stat reporting (percentiles comparing current readings).   

### Water Year  

Same as above, we can just call `parallel = TRUE` to run in parallel since we'll be getting peak flows from `dataRetrieval::readNWISpeak()`.  
```{r}
system.time({
pnw_wy <- suppressWarnings(suppressMessages(ww_wyUSGS(pnw_dv,
                                     parallel = TRUE)))
})
```

```{r, echo=F, warning = F, message=F, error=F}
 p1 <-  pnw_wy %>% 
  group_by(site_no) %>% 
  filter(!is.na(peak_va)) %>% 
  add_count() %>% 
  filter( n > 10) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(Kendall::MannKendall(.$peak_va))['p.value'])) %>% 
  unnest() %>% slice(n=1) %>% 
  ggplot(aes(p.value)) + 
  geom_histogram(aes(fill = ..x..)) +
  geom_vline(xintercept = 0.05, linetype = 2) + 
  geom_text(aes(x = 0.15, y = 50, label = '0.05')) +
  scale_fill_gradientn(colors = rev(hcl.colors('Zissou1', n = 32))) +
  theme_bw() + labs(fill = 'p.value')
  
p2 <- pnw_wy %>% 
  group_by(site_no) %>% 
  filter(!is.na(peak_va)) %>% 
  add_count() %>% 
  filter(n > 10) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(Kendall::MannKendall(.$peak_va))['p.value'])) %>% 
  unnest() %>% slice(n=1) %>% left_join(pnw_dv %>% group_by(site_no) %>% 
                                         dplyr::select(site_no, lat, long) %>% 
                                         slice(n=1), by = 'site_no') %>% 
  st_as_sf(coords = c('long', 'lat')) %>% 
  ggplot() + 
  geom_sf(aes(color = p.value), show.legend = F, size = .5)+
  scale_color_gradientn(colors = rev(hcl.colors('Zissou1', n = 32))) +
  borders('state', xlim = c(-130, -110), ylim = c(20, 50)) + 
  theme_bw() 

library(patchwork)
(p2|p1 + theme(legend.position = 'bottom', legend.margin = margin(l = -10, unit='cm'))) +
  plot_annotation(title = 'Mann-Kendall trend analysis (Peak Flow) with active USGS sites in Region 17', subtitle =  '10 years or more of data and drainage area < 2,000 km2')

p1 <-  pnw_wy %>% 
  group_by(site_no) %>% 
  filter(!is.na(peak_va)) %>% 
  add_count() %>% 
  filter( n > 10) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(Kendall::MannKendall(.$peak_va))['p.value'])) %>% 
  unnest() %>% 
   filter(p.value <= 0.05) %>% 
   group_by(Station) %>% 
   mutate(mean_q = mean(peak_va, na.rm = T)) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(lm(.$Flow_coef_var~.$wy, data = .))[2,2])) %>% 
  unnest() %>% ungroup() %>% 
   mutate(cut_q = cut_number(mean_q, n = 6),
          slope_col = if_else(estimate < 0, 'decrease', 'increase')) %>%  
   ggplot(aes(wy, peak_va)) + 
   geom_point(alpha = 0.05) + 
   stat_smooth(geom = 'line',alpha = 0.5,aes(group = site_no, color = slope_col),
               method = 'lm', se = F) +
   theme_bw() + 
   scale_y_continuous(labels = scales::comma) +
  facet_wrap(~cut_q, scales = 'free') + 
   labs(color = 'Slope (beta)', y = 'Peak Flows (cfs)',title = 'Sites with (p <= 0.05) and slope of linear model as color',
        subtitle = 'faceted by all-time mean peak flow')
 
p2 <- pnw_wy %>% 
  group_by(site_no) %>% 
  filter(!is.na(peak_va)) %>% 
  add_count() %>% 
  filter( n > 10) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(Kendall::MannKendall(.$peak_va))['p.value'])) %>% 
  unnest() %>% 
   filter(p.value <= 0.05) %>% 
   group_by(Station) %>% 
   mutate(mean_q = mean(peak_va, na.rm = T)) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(lm(.$Flow_coef_var~.$wy, data = .))[2,2])) %>% 
  unnest() %>% ungroup() %>% 
   mutate(cut_q = cut_number(mean_q, n = 6),
          slope_col = if_else(estimate < 0, 'decrease', 'increase'))%>% 
  group_by(site_no) %>% slice(n=1) %>% left_join(pnw_dv %>% group_by(site_no) %>% 
                                         dplyr::select(site_no, lat, long) %>% 
                                         slice(n=1), by = 'site_no') %>% 
  st_as_sf(coords = c('long', 'lat')) %>% 
  ggplot() + 
  geom_sf(aes(color = slope_col), show.legend = F, size = 1)+
  borders('state', xlim = c(-130, -110), ylim = c(20, 50)) + 
  theme_bw()


p1 
p2
```

### Without using parallel  

If you just want a single site, go for it! You don't always have to pipe a `ww_dvUSGS()` object into the `ww_*()` and can just use the `sites` argument. IMO its nice to have a `ww_dvUSGS()` object because you'll likely come back to it.  

```{r example}

yaak_min <- ww_wyUSGS(sites = '12304500')

ggplot(yaak_min, aes(wy, Flow_min)) +
  geom_point() + 
  geom_line() +
  theme_bw()
```

### Or minimum/maximum water temperature

```{r}

withlacoochee_temp <- ww_wyUSGS(sites="02319394",
                          parameter_cd = c("00010"))

ggplot(withlacoochee_temp, aes(wy, Wtemp_min)) +
  geom_point() + 
  geom_line() +
  theme_bw()

ggplot(withlacoochee_temp, aes(wy, Wtemp_max)) +
  geom_point() + 
  geom_line() +
  theme_bw()
```

