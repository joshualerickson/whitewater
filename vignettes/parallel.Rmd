---
title: "parallel"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{parallel}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Intro  

Running repetitive tasks in parallel can save a lot of time as well as provide more time to do other things like .
```{r setup}
library(whitewater)
```

### Running in parallel  

```{r, warning=F, message=F}
library(whitewater)
library(tidyverse)
library(sf)
library(future)
library(dataRetrieval)

huc17_sites <- dataRetrieval::whatNWISdata(huc = 17,
                                           siteStatus = 'active',
                                           service = 'dv',
                                           parameterCd = '00060',
                                           drainAreaMax = 2000)
cat("# of sites: ", nrow(huc17_sites))

st_as_sf(huc17_sites, coords = c('dec_long_va', 'dec_lat_va')) %>% 
  ggplot() + 
  geom_sf() +
  borders('state', xlim = c(-130, -110), ylim = c(20, 50)) + 
  theme_bw()
```


```{r}
#need to call future::plan()

##### Remember, please use 10 or less workers #####
plan(multisession(workers = 10))

#running on 11 cores

system.time({
pnw_dv <- ww_dvUSGS(huc17_sites$site_no,
                    parameter_cd = '00060',
                    wy_month = 10,
                    parallel = TRUE,
                    verbose = FALSE)
})

nrow(pnw_dv)

pnw_dv
```

Now we can use other `ww_` functions to filter the data by water year, month, water year and month, as well as stat reporting (percentiles comparing current readings).   

### Water Year  

Same as above, we can just call `parallel = TRUE` to run in parallel since we'll be getting peak flows from `dataRetrieval::readNWISpeak()`.  
```{r}

pnw_dv_filt <- pnw_dv %>% filter(obs_per_wy > 360)

system.time({
pnw_wy <- suppressMessages(ww_wyUSGS(pnw_dv_filt, parallel = TRUE, verbose = F))
})

pnw_wy
```


```{r, echo=F, warning = F, message=F, error=F}
 p1 <-  pnw_wy %>% 
  group_by(site_no) %>% 
  filter(!is.na(peak_va)) %>% 
  add_count() %>% 
  filter( n > 10) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(Kendall::MannKendall(.$peak_va))['p.value'])) %>% 
  unnest() %>% slice(n=1) %>% 
  ggplot(aes(p.value)) + 
  geom_histogram(aes(fill = ..x..)) +
  geom_vline(xintercept = 0.05, linetype = 2) + 
  geom_text(aes(x = 0.15, y = 50, label = '0.05')) +
  scale_fill_gradientn(colors = rev(hcl.colors('Zissou1', n = 32))) +
  theme_bw() + labs(fill = 'p.value')
  
p2 <- pnw_wy %>% 
  group_by(site_no) %>% 
  filter(!is.na(peak_va)) %>% 
  add_count() %>% 
  filter(n > 10) %>% 
  nest() %>% 
  mutate(model = map(data, ~broom::tidy(Kendall::MannKendall(.$peak_va))['p.value'])) %>% 
  unnest() %>% slice(n=1) %>% 
  st_as_sf(coords = c('long', 'lat')) %>% 
  ggplot() + 
  geom_sf(aes(color = p.value), show.legend = F, size = .5)+
  scale_color_gradientn(colors = rev(hcl.colors('Zissou1', n = 32))) +
  borders('state', xlim = c(-130, -110), ylim = c(20, 50)) + 
  theme_bw() 

library(patchwork)
(p2|p1 + theme(legend.position = 'bottom', legend.margin = margin(l = -10, unit='cm'))) +
  plot_annotation(title = 'Mann-Kendall trend analysis (Peak Flow) with active USGS sites in Region 17', subtitle =  '10 years or more of data and drainage area < 2,000 km2')
```


```{r}
#install latest dev version of sfdep 
#remotes::install_github("josiahparry/sfdep")

library(sfdep)

pnw_wy_filt <- read_csv('C:/Users/joshu/OneDrive/Documents/R/test/playing/pnw_wy_filt.csv') %>% 
               mutate(site_no = as.character(site_no)) %>% filter(wy >= 1991, wy < 2021)
pnw_wy_filt_sites <- pnw_wy_filt %>% pull(site_no) %>% unique()
huc_sites <- st_as_sf(huc17_sites, coords = c('dec_long_va', 'dec_lat_va')) %>% 
             filter(site_no %in% pnw_wy_filt_sites) 
huc_sites_dup <- duplicated(huc_sites$site_no)
huc_sites <- huc_sites[-huc_sites_dup,] %>% 
             st_as_sf() %>% st_set_crs(4326)
d = st_is_within_distance(huc_sites, , 0.01)
dupl = unlist(mapply(function(x,y) x[x < y], d, seq_along(d)))
huc_sites <- huc_sites[-dupl, ]%>% 
             st_as_sf() %>% st_set_crs(4326)

pnw_wy_filt <- pnw_wy_filt %>% filter(site_no %in% huc_sites$site_no)

pnw_wy_filt <- pnw_wy_filt %>% select(site_no, peak_va, wy) %>% filter(!is.na(peak_va)) %>% group_by(site_no) %>% 
               add_count() %>% ungroup() %>% filter(n >= 30)

pnw_wy_filt %>% group_by(site_no) %>% count() %>% view()

huc_sites <- huc_sites %>% filter(site_no %in% unique(pnw_wy_filt$site_no))

pnw_wy_spacetime <- spacetime(pnw_wy_filt,.geometry =  huc_sites,.loc_col =  'site_no',.time_col = 'wy')
pnw_wy_spacetime <- complete_spacetime_cube(pnw_wy_spacetime)

ehsa <- emerging_hotspot_analysis(
  x = pnw_wy_spacetime, 
  .var = "peak_va", 
  k = 1, 
  nsim = 99
)

ehsa %>% count(classification)

ehsa %>% 
  left_join(huc_sites, by = c('location' = 'site_no')) %>% 
  st_as_sf() %>% 
  ggplot() + 
  geom_sf(aes(color = classification), size = 3.25, color = 'black')+
  geom_sf(aes(color = classification), size = 2.5)+
  scale_color_manual(values = rev(hcl.colors('Reds', n = 6))) +
  borders('state', xlim = c(-130, -110), ylim = c(40, 50)) + 
  coord_sf( xlim = c(-125, -110), ylim = c(41, 50)) + 
  theme_void() 

ehsa <- emerging_hotspot_analysis(
  x = pnw_wy_spacetime, 
  .var = "peak_va", 
  k = 1, 
  nsim = 99
)

ehsa %>% count(classification)

ehsa %>% 
  left_join(huc_sites, by = c('location' = 'site_no')) %>% 
  st_as_sf() %>% 
  ggplot() + 
  geom_sf(aes(color = classification), size = 3.25, color = 'black')+
  geom_sf(aes(color = classification), size = 2.5)+
  scale_color_manual(values = rev(hcl.colors('Reds', n = 6))) +
  borders('state', xlim = c(-130, -110), ylim = c(40, 50)) + 
  coord_sf( xlim = c(-125, -110), ylim = c(41, 50)) + 
  theme_void() 
```



